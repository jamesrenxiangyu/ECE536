{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[A3]_ECE_471_536_Spring_2021_Assignment_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesrenxiangyu/ECE536/blob/main/%5BA3%5D_ECE_471_536_Spring_2021_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0nrIg51CXFF"
      },
      "source": [
        "![UVic logo](https://res-2.cloudinary.com/crunchbase-production/image/upload/c_lpad,h_256,w_256,f_auto,q_auto:eco/v1406151713/wptak6xuezyh36b1hbty.png)\n",
        "\n",
        "# **ECE 471/536 Spring 2021: Computer Vision**\n",
        "## Assignment 3: Visual Features Extraction and Classification \n",
        "### Due date: March 16, 10:00 PM PST\n",
        "\n",
        "\n",
        "> Student: Xiangyu Ren, V00941916\n",
        "---\n",
        "Abstract: *This assignment provides students with the opportunity to work with the various steps of a machine learning-based image classification pipeline: dataset curation and division, visual features extraction, classification, performance evaluation and fine-tuning.*\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srHd9KhbFHcU"
      },
      "source": [
        "## **1. Instructions:** follow the intructions provided on a sequential manner. \n",
        "### 1.0 **Identification** \n",
        "Please enter your name and V number on the text cell above.\n",
        "\n",
        "### 1.1 **Submission package**\n",
        "Your final submission package must be submitted using the [BrightSpace](https://https://bright.uvic.ca/d2l/home)  platform. You will find this assignment's specific page under **Course Tools > Assignments**. Your submission package consists of a *.zip* file containing:\n",
        "\n",
        "1.   *.ipynb* file: your modified version of this Google Colab template. Place your complete assignment solution/information in this version. \n",
        "\n",
        "### 1.2 **Coding considerations**\n",
        "* In previous years we asked students to complete assignments offline by installing either MATLAB or a Python environment in their computers. In order to standardize the submissions and guarantee that everyone has access to the same Python environment, all assignments are going to be described (by us) and completed (by you) using the same Google Colab template script.\n",
        "* Google Colab offers a Python environment that can be accessed in your browser and executed using Google Cloud, so no local installation is necessary. It makes the setting-up process significantly easier! Please read [this quick tutorial](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb) notebook on Google Colab/Python.\n",
        "\n",
        "### 1.3 **Creating your Google Colab submission file**\n",
        "\n",
        "* Modify this template with your solutions to the assignment. You will find \"**TO-DO**\" indicators throughout the assingment highlighting portions of the code you are asked to complete, as well as their worth. \n",
        "* Colab notebooks are divided into individual cells. You can execute the code inside of a given cell by pressing **CTRL+ENTER**, or that of all cells by pressing **CTRL+F9**. Variables must be \"executed\" in a cell before being used by subsequent ones (the same goes for libraries imported). Note that some cells of this assignment contain flags that must by changed (and executed) before you move forward.\n",
        "* If you completed the whole assignment, make sure that simply pressing \"**CTRL+F9**\" executes all cells correctly. **This is going to be the first marking step we will execute when evaluating your submission**.  \n",
        "\n",
        "### 1.4 **Use of open source code**\n",
        "\n",
        "* The use of small segments of freely-available code is permitted. However, it is **extremely important** that you indicate in your in-code comments where these are used, as well as their sources. Failure to do so can be considered plagiarism, which is a serious offence. Learn more about detection mechanisms and consequences of plagiarism at UVic [here](https://www.uvic.ca/library/research/citation/plagiarism/). Note that the programming assignments are designed so that most of their content should be created by you.     \n",
        "* A number of functions/algorithms are already implemented by libraries we will use (e.g., [OpenCV](https://opencv.org/), [scikit-learn](https://scikit-learn.org/stable/)), however you should not use them unless otherwise instructed to do so. Mannualy coding some of these function is an important part of the learning process. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQS1k9BUa_8Z"
      },
      "source": [
        "READ_THE_INSTRUCTIONS_FLAG = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRhMh6tJjvcc"
      },
      "source": [
        "# **2. Programming: Dataloading, feature extraction, model training and evaluation for image classification (75 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D4ecEgtjzOY"
      },
      "source": [
        "### **2.1 Dataloading: reading, shuffling and pre-processing the data**\n",
        "\n",
        "Different datasets need to be handled distinctly. Often you will need to code your custom dataloader to correctly retrieve the data and its labels following a given format. Although a number of libraries offer easier ways to load data (e.g., [sklearn](https://scikit-learn.org/0.16/datasets/index.html), [torchvision](https://pytorch.org/vision/0.8/datasets.html)), we are going to code a custom dataloading scheme based on a simple dataset of images from three classes. This pseudo-dataloder is going to read sub-folders from a root directory and create data and labels structures based on them. Our dataloading process is also going to involve the **shuffling** of the training and testing sets.  \n",
        "\n",
        "The **train** and **test** subsets (folders) are already provided. Note that one would typically load a complete, unified dataset and randomly partition it into training, validation and testing subsets. In fact, distinct splits of a single dataset into different subsets are the basis of **cross-fold validation**. The **validation** subset helps evaluating the performance of the system while the training process takes place without influencing in its parameter's values (as the samples in the training subset are supposed to do via backpropagation). We are going to create and work with a validation subset only in the next assignment, resorting to only train and test sets on the current one.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_yOTMF-S_e-"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from skimage import data, exposure\n",
        "from google.colab import files\n",
        "import sys\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "print('-'*40)\n",
        "print ('Python version: {}'.format(sys.version))\n",
        "print('OpenCV version: {}'.format(cv2.__version__))\n",
        "\n",
        "if not READ_THE_INSTRUCTIONS_FLAG:\n",
        "  raise Exception('Please go back and read the instructions.')\n",
        "else:\n",
        "  print('\\nThank you for reading the instructions.')\n",
        "print('-'*40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdOyjwThadrE"
      },
      "source": [
        "# TO-DO: create the function \"read_data\", which reads all .jpg images from the subfolders of a given \r\n",
        "# path (i.e., root folder). Follow the template provided. \r\n",
        "\r\n",
        "def read_data(address, resize_dim = (64,64)): # the default resizing dimensions are 64 x 64 pixels\r\n",
        "\r\n",
        "    # This function reads all images from a folder (whose address is given by input \"address\")\r\n",
        "    # and its subfolders, and creates labels based on the subfolder names. Consider the \r\n",
        "    # following illustrative directory structure from a root folder called \"train\" or \"test\"\r\n",
        "    # with subfolders named \"class1\" and \"class2\" (two classes):\r\n",
        "    #\r\n",
        "    # +-- train\r\n",
        "    #     +-- class1\r\n",
        "    #     +-- class2\r\n",
        "    # +-- test\r\n",
        "    #     +-- class1\r\n",
        "    #     +-- class2\r\n",
        "    #\r\n",
        "    # In this example, we want the function to, given the path to \"train\" (or \"test\"),\r\n",
        "    # visit sub-folders \"class1\" and \"class2\" and store all of their images into a single ndarray.\r\n",
        "    # While doing that, the function must also create a \"labels\" ndarray specifying the  \r\n",
        "    # class each sample belongs to. \r\n",
        "\r\n",
        "\r\n",
        "    # 0. Grab folders/sub-folders addresses and number of classes (i.e., # of subfolders)\r\n",
        "    # tip: use os.walk\r\n",
        "    folders = # TO-DO # structure with folders and subfolders\r\n",
        "    subfolderNames = #TO-DO # subfolders names. index [0] is the root directory.\r\n",
        "    n_classes = # TO-DO # number of subfolder = number of classes.\r\n",
        "\r\n",
        "    # 1. Create empty lists named \"data\" and \"labels\". Create an empty dictionary named\r\n",
        "    # \"classes_labels\". \r\n",
        "    # Dictionaries are convenient data structures offered in Python to create\r\n",
        "    # key-value pairs. The \"classes_labels\" dictionary will specify the index-to-class\r\n",
        "    # correspondence. E.g., classes_labels['0'] = \"beaver\", classes_labels['1'] = \"moose\", etc. \r\n",
        "    data = # TO-DO\r\n",
        "    labels = # TO-DO\r\n",
        "    classes_labels = # TO-DO (create an initially EMPTY dictionary)\r\n",
        "\r\n",
        "    # 2. Create a loop that does the following: \r\n",
        "    #   2.1. Visits each sub-folder of the path provided and,\r\n",
        "    #   2.2. For each new subfolder, creates a key/value pair in the \"classes_labels\"\r\n",
        "    #     dictionary (see example above).\r\n",
        "    #   2.3  Reads an individual .jpg image file using the cv2 library.\r\n",
        "    #   2.4  Resizes the image using the dimensions specified by the \"resize_dim\"\r\n",
        "    #     input. \r\n",
        "    #   2.3. Adds each resized image to an individual index of the \"data\" list\r\n",
        "    #     (all images are added to the same \"data\" list)\r\n",
        "    #   2.4. For each new file read, add the corresponding label to the \"labels\"\r\n",
        "    #     list. E.g., consider two data folders (two classes) with 5 and 10 samples,\r\n",
        "    #     respectively. The \"labels\" list would have the following values (in each position): \r\n",
        "    #     000001111111111\r\n",
        "\r\n",
        "    for i in range(n_classes): # for each subfolder,\r\n",
        "\r\n",
        "        # classes_labels = #TO-DO # create an \r\n",
        "        # entry in the dictionary for the new class.\r\n",
        "\r\n",
        "        # for each image in a subfolder:\r\n",
        "            \r\n",
        "            # add a label for this class in the labels list\r\n",
        "            # read the image\r\n",
        "            # resize the image and change BGR to RGB (for matplotlib plotting purposes)\r\n",
        "            # add this image to the \"data\" list\r\n",
        "\r\n",
        "    # 3. Convert the \"data\" and \"labels\" lists into numpy ndarrays.\r\n",
        "    data = # TO-DO # convert data into NxHxWxC, where N = # of samples, H=height, W=width, C = color channels\r\n",
        "    labels = # TO-DO # convert labels list into np ndarray\r\n",
        "\r\n",
        "    # 4. Return the \"data\" and \"labels\" ndarrays, as well as the \"classes_labels\" dictionary.\r\n",
        "    return data, labels, classes_labels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2LOLS3MExES"
      },
      "source": [
        "# TO-DO: check if the \"./data/train\" or \"./data/test\" folders already exist in your \r\n",
        "# colab repository. If they don't, ask the user to upload a file called \"data.zip\"\r\n",
        "# and unzip it. This file posses the aforementioned folders.\r\n",
        "# note: use the \"data.zip\" file provided in your assignment description. It has \r\n",
        "# ~35MB. Allow some minutes for it to be uploaded to your colab repository. If\r\n",
        "# the colab session is disconnected, you will need to upload the .zip file again.\r\n",
        "\r\n",
        "# TO-DO: check if \"./data/train\" and \"./data/test\" exist. If not, ask user to upload \r\n",
        "# \"data.zip\" and unzip it. This file will have the aforementioned sub-folders.\r\n",
        "\r\n",
        "train_folders = \"./data/train/\"\r\n",
        "test_folders = \"./data/test/\"\r\n",
        "resize_dims = (64,64) # determines the target resizing dimensions.\r\n",
        "\r\n",
        "# TO-DO: use your \"read_data\" function to read the train and test folders/files. You might \r\n",
        "# need to call this function twice (once for the train folder, once for the test folder).   \r\n",
        "train_data, train_labels, classes_labels = # TO-DO\r\n",
        "test_data, test_labels, classes_labels_test = # TO-DO \r\n",
        "\r\n",
        "# assertion to guarantee that the train and test classes are the same based on the subfolders' names.\r\n",
        "assert classes_labels==classes_labels_test, 'Your train/test folder names do not match'\r\n",
        "\r\n",
        "# TO-DO: print thte shape of the train and test ndarrays. If correctly loaded, the dimensions \r\n",
        "# of the two data lists should be (253, 64, 64, 3) for train, and (54, 64, 64, 3) for test. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X02pDcfPjapx"
      },
      "source": [
        "# TO-DO: create the function \"randomize_and_display\", which randomizes the train or test datasets,\r\n",
        "# and display some samples with their associated labels to check its results. Follow the template provided. \r\n",
        "\r\n",
        "def randomize_and_display(data, labels, classes_labels, display_n=5, phase=None):\r\n",
        "\r\n",
        "  # data: dataset (e.g., test or train data lists)\r\n",
        "  # labels: list of labels for your dataset\r\n",
        "  # classes: dictionary that specifies the correspondence of classes and labels\r\n",
        "    # (see the template of function \"read_data\" for details)\r\n",
        "  # display_n: number of randomized samples to display (default: 5)\r\n",
        "  # phase: either train or test (for debugging purposes)\r\n",
        "  \r\n",
        "    # 0. Print the type of data being shuffled (either \"train\" or \"test\").\r\n",
        "    if phase is not None:\r\n",
        "      print(\"\\nShuffling {} data\".format(phase))\r\n",
        "\r\n",
        "    # 1. shuffle both the data and its equivalent labels in the same way  \r\n",
        "    # (So that the label/data correspondence is maintained).\r\n",
        "    # if using a shuffling algorithmn, use \"10\" as a seed (for consistency of\r\n",
        "    # results). tip: sklearn's \"shuffle\" function. \r\n",
        "    data_shuff, labels_shuff = # TO-DO\r\n",
        "        \r\n",
        "    # 2. create a plot showing the first \"display_n\" number of images from the shuffled\r\n",
        "    # train or test sets. The title of each image should display its corresponding\r\n",
        "    # label (using the \"labels\" and \"classes_labels\" inputs). You should see samples\r\n",
        "    # of all three classes even if choosing a relatively small subset (e.g., display_n>=4). \r\n",
        "    # Hide the axis of the plot for visibility. An example of how your \r\n",
        "    # plot should look like after calling the function twice (once for train, once for test data) is provided at:\r\n",
        "    # https://raw.githubusercontent.com/tunai/storage/master/images/teaching/ece%20473-536/A3/shuffled_data.jpg  \r\n",
        "\r\n",
        "    fig= #TO-DO # create the figure element\r\n",
        "    for i in range(display_n): \r\n",
        "      # add an image to the figure element to be displayed\r\n",
        "      # with the correct title on top of it (see example above)\r\n",
        "\r\n",
        "\r\n",
        "    # 3. return the shuffled data and labels\r\n",
        "    return data_shuff, labels_shuff\r\n",
        "\r\n",
        "# TO-DO: use your \"randomize_and_display\" function to shuffle your train and test subsets, as\r\n",
        "# well as their corresponding labels.\r\n",
        "# Note: a \"display_n\" number of images should be displayed to guarantee that the shuffling was correctly completed.\r\n",
        "# Again, an example of how your plots should look like after calling the function twice is provided at:\r\n",
        "# https://raw.githubusercontent.com/tunai/storage/master/images/teaching/ece%20473-536/A3/shuffled_data.jpg\r\n",
        "\r\n",
        "train_data, train_labels = # TO-DO # use randomize_and_display on the train data\r\n",
        "test_data, test_labels = # TO-DO # use randomize_and_display on the test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjSnvDLUAesV"
      },
      "source": [
        "### **2.2 Feature extraction: HOG, Hue Histogram and Raw RGB**\r\n",
        "\r\n",
        "Now that your train and test data are correctly loaded and pre-processed, you need to extract meaningful visual features from them. These features are going to drive machine lenarning-based image classifications tasks. Since such classifiers are trained and tested based on the features extracted, their design and quality (i.e., how well they represent/generalize the data) are paramount in the performance of the system.\r\n",
        "\r\n",
        "In this assignment we are going to work with three generic features (i.e., not specific to a certain target class) and analyze the classification performance based solely on each of them. Note that recent deep learning-based approaches also calculate similar features, but mostly using convolutional kernels in an efficient [1] manner.\r\n",
        "\r\n",
        "[1] Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. Communications of the ACM. 2017 May 24;60(6):84-90."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk9UCiN_CcD8"
      },
      "source": [
        "# 2.2.1. Histogram of Oriented Gradients\r\n",
        "\r\n",
        "# TO-DO (x points): create a function (\"my_hog_batch\") that receives an N x H x W x C numpy ndarray \r\n",
        "# of data and calculates, for each image, a row vector representing its HOG. \r\n",
        "# The final output matrix represents the HOG features of all images in the input dataset\r\n",
        "# put together (i.e., a row—HOG—per image).\r\n",
        "\r\n",
        "def my_hog_batch(data, phase = None):\r\n",
        "  \r\n",
        "  # 0. Grab the features of a single sample to determine the shape of the HOG row vector\r\n",
        "  # You do not need to implement the HOG algorithm again. A tip is to use skimage's \"hog\" function.\r\n",
        "  # use the following parameters: 9-bin HOGs, cells of 8 x 8 pixels, blocks of 3 x 3 cells. \r\n",
        "\r\n",
        "  template = # TO-DO # Calculates the HOG features of a single sample\r\n",
        "  hog_size = # TO-DO # Grabs the dimensions of the template just calculated\r\n",
        "  \r\n",
        "  # 1. Print what type of data is being processed (for debugging purposes)\r\n",
        "  if phase is not None:\r\n",
        "      print(\"Extracting HOG features from the {} dataset...\".format(phase))\r\n",
        "  else:\r\n",
        "      print(\"Extracting HOG features...\")\r\n",
        "\r\n",
        "  # 2. Pre-allocate with zeros a numpy ndarray of dimensions N (samples) x hog_size to receive the HOG features\r\n",
        "  hog_features = # TO-DO # pre-allocate with zeros\r\n",
        "\r\n",
        "  # 3. loop through each sample in the dataset\r\n",
        "  \r\n",
        "  for i in range (data.shape[0]):\r\n",
        "    \r\n",
        "      # 4. grab a single sample and turn it to grayscale\r\n",
        "      current = # TO-DO \r\n",
        "      \r\n",
        "      # 5. calculate the HOG features for the grayscale sample. Again, use the following parameters:\r\n",
        "      # orientation bins: 9, cell size: 8x8 pixels, blocks: 3x3 cells\r\n",
        "      # Each row of your \"hog_features\" is going to hold the flattened (i.e., row vector) \r\n",
        "      # output of this function. \r\n",
        "      # tip: use skimage's \"hog\" function\r\n",
        "\r\n",
        "      # TO-DO: update the value of \"hog_features\" with a new row (i.e., HOG of a new image)      \r\n",
        "      \r\n",
        "  \r\n",
        "  return hog_features\r\n",
        "\r\n",
        "# TO-DO: use the \"my_hog_batch\" function to extract the HOG features from both \r\n",
        "# train and test datasets.\r\n",
        "# Save your HOG train and test features in a dictionary named\r\n",
        "# \"features\" under keys \"HOG_train\" and \"HOG_test\".\r\n",
        "\r\n",
        "features = {}\r\n",
        "#TO-DO: create the \"HOG_train\" and \"HOG_test\" keys and add the output of your \r\n",
        "# \"my_hog_batch\" function to them. \r\n",
        "\r\n",
        "# assertions to make sure that the dimensions of your HOG features matrices are correct\r\n",
        "assert (features['HOG_test'].shape[0]==test_data.shape[0]) and (features['HOG_train'].shape[0]==train_data.shape[0]), 'Dimensions of HOG features matrix wrong.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH1-Pas_OtA9"
      },
      "source": [
        "# 2.2.2. Hue Histogram \r\n",
        "\r\n",
        "# TO-DO (x points): create a function that receives an N x H x W x C numpy ndarray \r\n",
        "# of data and calculates, for each image, a row vector representing a histogram of \r\n",
        "# hue values. \r\n",
        "# The final output matrix represents, in each row, the hue histogram of one single\r\n",
        "# image from the input dataset.\r\n",
        "\r\n",
        "def my_hhist_batch(input, n_bins, phase = None):   \r\n",
        "  \r\n",
        "  # 0. Print what type of data is being processed (for debugging purposes).\r\n",
        "  if phase is not None:\r\n",
        "      print(\"Extracting h histogram features from the {} dataset...\".format(phase))\r\n",
        "  else:\r\n",
        "      print(\"Extracting h histogram...\")\r\n",
        "\r\n",
        "  # 1. pre-allocate two matrices: \r\n",
        "  # 1) the matrix of HSV images to be converted from the input data matrix; \r\n",
        "  # 2) the matrix of histograms to be calculated from the hue channel of each image\r\n",
        "  hsv = # TO-DO # pre-allocate with zeros\r\n",
        "  h_hist = # TO-DO # pre-allocate with histograms with \r\n",
        "  # zeros (use n_bins to determine the number of bins of these histograms). \r\n",
        "  # recall: one histogram per image\r\n",
        "\r\n",
        "  # 2. loop through each sample in the dataset\r\n",
        "  for i in range(input.shape[0]):\r\n",
        "\r\n",
        "      # 3. Convert the current RGB image to HSV\r\n",
        "      current = #TO-DO\r\n",
        "\r\n",
        "      # 4. Calculate the Hue histogram of the HSV version of the current image and \r\n",
        "      # save it as a row of your \"h_hist\" matrix (i.e., consider only the first \r\n",
        "      # channel of the HSV image). Limit the histogram to the [0,1] range. \r\n",
        "      # The number of bins is determined by the \"n_bins\" input hyper parameter. \r\n",
        "      # Note: you only need to store the values of each bin (rather than the \r\n",
        "      # values and bin edges). For example: for 10-bin histograms, each HSV image will \r\n",
        "      # result in a 1x10 row vector output.  \r\n",
        "      # tip: you can use numpy's histogram-calculating function.\r\n",
        "\r\n",
        "      #TO-DO: update the value of h_hist\r\n",
        "\r\n",
        "  # return the output matrix\r\n",
        "  return h_hist\r\n",
        "\r\n",
        "# TO-DO: use your \"my_hhist_batch\" function to extract the Hue Histogram features\r\n",
        "# from both train and test datasets. Create 16-bin histograms. \r\n",
        "# these features should be saved on your \"features\" dictionary under keys \r\n",
        "# \"HHist_train\" and \"HHist_test\"\r\n",
        "n_bins = 16 \r\n",
        "#TO-DO: create the \"HHist_train\" and \"HHist_test\" keys and add the output of your \r\n",
        "# \"my_hhist_batch\" function to them. \r\n",
        "\r\n",
        "\r\n",
        "# assertions to make sure that the dimensions of your features are correct\r\n",
        "assert (features['HHist_train'].shape[0]==train_data.shape[0]) and (features['HHist_test'].shape[0]==test_data.shape[0]), 'Dimensions of HHist features matrix wrong.'\r\n",
        "assert (features['HHist_train'].shape[1]==features['HHist_test'].shape[1]==n_bins), 'Dimensions of HHist features matrix wrong.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSmLXmjF_BDE"
      },
      "source": [
        "# 2.2.3. Raw RGB features \r\n",
        "\r\n",
        "# TO-DO (x points): as a last set of visual features, simply flatten (i.e., turn into a \r\n",
        "# row vector) all pixel intensities found on the images of the datasets.\r\n",
        "# each image is going to be presented as a single row in this feature matrix.\r\n",
        "# Cast the intensity values as floats for further manipulations. \r\n",
        "# tip: for N color images of dimensions H x W, your raw RGB values \r\n",
        "# matrix should be of dimensions N,(HxWx3)\r\n",
        "# these raw features should be saved on your \"features\" dictionary under keys \r\n",
        "# \"raw_train\" and \"raw_test\"\r\n",
        "\r\n",
        "# assertion to make sure that the dimensions of one of the matrices are correct\r\n",
        "assert(features['raw_train'].shape[1]==(train_data.shape[1]*train_data.shape[2]*train_data.shape[3])),'Wrong RGB raw values matrix dimensions'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHVe5yUrQ9a2"
      },
      "source": [
        "### **2.3 Creating and training the classifiers**\r\n",
        "\r\n",
        "We are going to test the efficacy of three classifiers using our feature matrices: \r\n",
        "- Support Vector Machine (SVM)\r\n",
        "- Logistic Regresion (also know as maximum-entropy classification, log-linear classifier)\r\n",
        "- Gaussian Naive Bayes\r\n",
        " \r\n",
        "Easy-to-use implementation of these models are made available by the scikit-learn module. \r\n",
        "\r\n",
        "Note that we are working with a **classification** task: the independent variables (i.e., features) are going to determine the probability that a discrete value (class) happens. In the end, the dependent variable (output) can only assume a fixed number of values—the classes of the training data (for our datasets of three classes: SRKW, beaver and moose, \"3\"). Another common machine learning-based task is **regression**, where the independent variables result in a *continuous* value for the dependent variable (output). A common supervised machine learning algorithm used in such cases is the **linear regression**, where a line is fit to the data to map the values from the features to the predicted, continuous output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-GnYalDUpSh"
      },
      "source": [
        "# TO-DO: import the necessary modules from sklearn to create the three\r\n",
        "# models (svm, naive bayes and linear regression).\r\n",
        "\r\n",
        "# TO-DO: create a function (\"my_model_trainer\") that receives a feature vector\r\n",
        "# and corresponding labels, trains and returns a model specified by the user.\r\n",
        "# Follow the template provided. \r\n",
        "\r\n",
        "def my_model_trainer(features, labels, model='nb'):\r\n",
        "\r\n",
        "  # features: matrix of features previously calculated\r\n",
        "  # labels: labels of the data\r\n",
        "  # model: model to be created. must be have of three possible values:\r\n",
        "  # \"gauss_nb\" (default), \"lreg\" or \"svm\" \r\n",
        "  \r\n",
        "  # 1. Based on the user choice (\"model\"), create one of \r\n",
        "  # Gaussian Naive Bayes, Logistic Regression or Support Vector\r\n",
        "  # Machine models. \r\n",
        "\r\n",
        "  # Note: for SVM, use \"svm.SVC\" with gamma='auto', linear kernel, and \r\n",
        "  # one-vs-rest multiclass decision function.\r\n",
        "\r\n",
        "  # example of a model:\r\n",
        "  if model=='nb':\r\n",
        "    # 1.1 use scikit-learn to declare a GaussianNB model\r\n",
        "    print('Creating a Gaussian Naive Bayes Model...')\r\n",
        "    model = #TO-DO\r\n",
        "  \r\n",
        "    # TO-DO: create the conditionals for the other two models\r\n",
        "      \r\n",
        "  # 2. train the model using the feature vector matrices and labels provided\r\n",
        "  # as inputs\r\n",
        "  # tip: use the \"fit\" function from sklearn\r\n",
        "  \r\n",
        "  print('Finished training the model.')\r\n",
        "\r\n",
        "  # 3. return the model created and trained \r\n",
        "  return model\r\n",
        "\r\n",
        "# TO-DO: create all nine possible models by simply changing the input parameters of your\r\n",
        "# \"my_model_trainer\" function.\r\n",
        "# i.e., model 1: svm using HOG, model 2: svm using H Hist, ... , model 9: logistic regression\r\n",
        "# using raw pixel intensities. \r\n",
        "\r\n",
        "# You must store each model as a key/value pair in a dictionary called \"models\".\r\n",
        "# Instead of mannualy calling your \"my_model_trainer\" nine times, you are asked \r\n",
        "# to use loops (for a maximum of NINE iterations) to populate this dictionary.\r\n",
        "\r\n",
        "# The keys of the \"models\" dictionary should be a combination of [\"svm\",\"nb\",\"lreg\"] and \r\n",
        "# [\"HOG\",\"HHist\",\"raw\"]. \r\n",
        "# e.g., \"svm_HOG\", \"lreg_raw\", ... (9 possibilities)\r\n",
        "\r\n",
        "models = {}\r\n",
        "model_type = #TO-DO (list with three strings specifying the possible model names)\r\n",
        "feature_type = #TO-DO (list with three strings specifying the possible feature types)\r\n",
        "\r\n",
        "# TO-DO: a loop (of a maximum of nine iterations) that creates nine models and place each of \r\n",
        "# them in a key/value pair of the \"models\" dictionary.\r\n",
        "\r\n",
        "# Note: you can ignore the warnings if some of your Logistic Regression models fail to converge \r\n",
        "(e.g., \"extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, TOTAL NO. of ITERATIONS REACHED LIMIT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzhSv1-uhxm2"
      },
      "source": [
        "### **2.4 Evaluating the classifiers**\r\n",
        "\r\n",
        "We will now create a function to calculate a number of metrics that will help evaluating the different pairs of classifier/features. The performance metrics this function calculates are: \r\n",
        "- Class-specific recall\r\n",
        "- Class-specific precision\r\n",
        "- Average recall\r\n",
        "- Average precision\r\n",
        "- Average F1-Score\r\n",
        "- Average Accuracy\r\n",
        "\r\n",
        "The auxiliary tool we are going to use to visulize the results and calculate performance metrics is the **confusion matrix**. \r\n",
        "Recall that a confusion matrix summarizes the predictions of a model while specifying if they were correct or not. The values of such matrix can be used to infer the number of True Positives (TP), True Negatives (TN), False Positives (FP) and False Negatives (FN), as discussed in Lecture 12."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzEiaoc7mPxD"
      },
      "source": [
        "# TO-DO: as a first analysis step, consider the SVM classifier models trained with\r\n",
        "# the three feature types (HOG, HHist and Raw). Use sklearn's \"plot_confusion_matrix\" and \r\n",
        "# \"confusion matrix\" to show the confusion matrix (non-normalized) using these three SVM models.\r\n",
        "# In other words, calculate and display three confusion matrices using the SVM classifier and the HOG, \r\n",
        "# HHist and raw TEST (not TRAIN) features/labels.\r\n",
        "# An example of one such confusion matrix is provided: \r\n",
        "# https://raw.githubusercontent.com/tunai/storage/master/images/teaching/ece%20473-536/A3/example_cm_svm_raw_feat.jpg\r\n",
        "  \r\n",
        "# Note: in total, you are asked to present THREE confusion matrices. Each of them will reflect 54 predictions (number of \r\n",
        "# samples in the TEST set)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTuhTr-0iOIs"
      },
      "source": [
        "# TO-DO: create a function called \"my_evaluation\" to systematically calculate \r\n",
        "# a model's confusion matrix and a number of evaluation metrics based on it. Follow\r\n",
        "# the template provided below.  \r\n",
        "\r\n",
        "def my_evaluation(features, labels, model):\r\n",
        "\r\n",
        "    # This function receives as inputs the TEST SET features, TEST SET labels and \r\n",
        "    # the model (trained on the TRAIN SET) to be evaluated. \r\n",
        "  \r\n",
        "    # Note: you CANNOT use \"sklearn.metrics\" methods in this function. The goal is that you\r\n",
        "    # calculate them manually from a confusion matrix.\r\n",
        "\r\n",
        "    # TO-DO: create a dictionary called \"results\" to hold six metrics.\r\n",
        "    # The keys of such dictionary should be \"accuracy\", \"recall\",\r\n",
        "    # \"precision\", \"avg_recall\", \"avg_precision\" and \"fscore\".\r\n",
        "    # The initial values of such key/value pair should be set to zero. \r\n",
        "\r\n",
        "    results = # TO-DO\r\n",
        "\r\n",
        "    # TO-DO: use the features and model provided as input parameters to make the\r\n",
        "    # classification predictions.\r\n",
        "    pred = # TO-DO\r\n",
        "\r\n",
        "    # TO-DO: create a confusion matrix based on the \"labels\" and \"pred\".\r\n",
        "    # hint: you may use sklearn.metrics.confusion_matrix to create this matrix.\r\n",
        "    # note: if using sklearn, note that it creates a confusion matrix where the\r\n",
        "    # y-axis represents the LABELS, while the x-axis represents\r\n",
        "    # the PREDICTIONS (conversely to what we did in class).\r\n",
        "    # e.g.,\r\n",
        "    # Consider labels = [2, 0, 2, 2, 0, 1], pred = [0, 0, 2, 2, 0, 2].\r\n",
        "    # The confusion matrix (CM) would be: array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])\r\n",
        "    # And the normalized CM would be: [[1, 0, 0], [0, 0, 1], [0.33, 0, 0.66]]\r\n",
        "\r\n",
        "    cm_raw = # TO-DO # calculate the raw (non-normalized) CM\r\n",
        "\r\n",
        "    # TO-DO: calculate the number of True Positives, False Negatives, \r\n",
        "    # and False Positives based on your confusion matrix (refer to the\r\n",
        "    # slides for Lecture 12). \r\n",
        "    # tip: consider these metrics as class-specific. Thus for N classes, each metric\r\n",
        "    # will be a row vector of dimensions 1,N. The \"average\" metrics will consider \r\n",
        "    # all classes at once. \r\n",
        "\r\n",
        "    TP = # TO-DO\r\n",
        "    FP = # TO-DO\r\n",
        "    FN = # TO-DO\r\n",
        "\r\n",
        "    # TO-DO: calculate the class-specific and average recall. insert\r\n",
        "    # each value in its corresponding key on the dictionary previously created\r\n",
        "    # tip: consider scenarios where a division by zero (i.e., NaN results)\r\n",
        "    # can happen. You might want to add very small values on denominators to\r\n",
        "    # avoid that.\r\n",
        "\r\n",
        "    # update the values of keys \"recall\" and \"avg_recall\" from the \"results\" dictionary\r\n",
        "\r\n",
        "    # TO-DO: calculate the class-specific and average precisions. insert\r\n",
        "    # each value in its corresponding key on the dictionary previously created.\r\n",
        "    # tip: consider scenarios where a division by zero (i.e., NaN results)\r\n",
        "    # can happen. You might want to add very small values on denominators to\r\n",
        "    # avoid that.\r\n",
        "\r\n",
        "    # update the values of keys \"precision\" and \"avg_precision\" from the \"results\" dictionary\r\n",
        "\r\n",
        "    # TO-DO: calculate the F1-Score and add it to its corresponding key in your\r\n",
        "    # \"results\" dictionary\r\n",
        "    \r\n",
        "    # TO-DO: calculate the accuracy and add it to its corresponding key in your\r\n",
        "    # \"results\" dictionary. \r\n",
        "    # Note: even without the TN element you can calculate the accuracy. Simply \r\n",
        "    # check the mean of \"pred==labels\"    \r\n",
        "    results[\"accuracy\"] = np.mean(pred == labels)\r\n",
        "\r\n",
        "    # provided checks to make sure that the manually calculated metrics are correct.\r\n",
        "    check1 = np.isclose(results['avg_recall'], recall_score(labels, pred, average='macro'))\r\n",
        "    check2 = np.isclose(results['avg_precision'], precision_score(labels, pred, average='macro'))\r\n",
        "    check3 = np.isclose(results['accuracy'], accuracy_score(labels, pred, normalize=True))\r\n",
        "\r\n",
        "    assert (check1 and check2 and check3), 'accuracy, precision or recall are not correct.'\r\n",
        "\r\n",
        "    # TO-DO: return your evaluation metrics \r\n",
        "    return results\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0sl9LbVACcx"
      },
      "source": [
        "# TO-DO: create a dictionary called \"results\" and save, on each \r\n",
        "# key/value pair, the results of the evaluation from each of the \r\n",
        "# nine models trained. To focus our analisys, you only need to \r\n",
        "# record the average precision, average recall and F1-score in this dictionary. \r\n",
        "# Use loops to fill in the dictionary using a maximum of NINE iterations.\r\n",
        "\r\n",
        "data = np.zeros((9,3)) # only three metrics and nine possible models \r\n",
        "results = {}\r\n",
        "idx = 0\r\n",
        "\r\n",
        "# TO-DO: loop throughout each model and feature type (total of nine iterations). \r\n",
        "# For each model/feature index,\r\n",
        "# 1) Use your \"my_evaluation\" function to calculate the performance metrics. Save the \r\n",
        "# result on the \"results\" dictionary with the following key name template: \"results[m+\"_\"+f+\"_eval\"]\"\r\n",
        "# where \"m\" is the name of the model, and \"f\" is the type of the feature.   \r\n",
        "# e.g., For SVM using HOG, the performance metrics would be saved on \"results[svm_HOG_eval]\".\r\n",
        "# 2) fill data[idx,0], data[idx,1] and data[idx,2] with the values of \"avg_recall\", \"avg_precision\" and \r\n",
        "# \"fscore\" for this model/features pair.   \r\n",
        "\r\n",
        "\r\n",
        "# TO-DO: show the average precision, average recall and F1-score metrics of each \r\n",
        "# of the nine model/feature pairs in a single table. \r\n",
        "# The row and column labels are provided below. \r\n",
        "# Tip: you can use plt.table\r\n",
        "# as a reference, your table should look [like this](https://raw.githubusercontent.com/tunai/storage/master/images/teaching/ece%20473-536/A3/example_matrix.jpg):\r\n",
        "\r\n",
        "row_labels = ['SVM_HOG','SVM_HHist','SVM_Raw','NB_HOG','NB_HHist','NB_Raw','LogR_HOG','LogR_HHist','LogR_Raw']\r\n",
        "col_labels = [\"AvgR\", \"AvgP\", \"F1\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttyVbJgqTS6z"
      },
      "source": [
        "### **2.5 Increasing the classification performance**\r\n",
        "\r\n",
        "There exists a number of design choices to make when curating a dataset and training a model. These include, but are not limited to: \r\n",
        "\r\n",
        "- Size of the input images (if the data is resized)\r\n",
        "- Data standardization: turn the data (either raw of feature-like) into a zero-mean, unit variance set\r\n",
        "- Data augmentation: modify the input data to train the models using more generic and representative data\r\n",
        "- Solvers: supervised machine learning algorithms use specific techniques to carry out their optimization problems. Changing these techniques influences in the models' performance.   \r\n",
        "\r\n",
        "The \"sklearn\" library provides functions to easily implement these pre-processing and training mechanisms. Note that different combinations of them might **improve or decrease** the performance of the models  \r\n",
        "\r\n",
        "You are asked to test different layouts and use the evaluation pipeline created up until sub-section 2.4 to analyze each of them. **Your goal is to reach a layout that surpasses 0.84 of F1-score using any combination of the aforementioned modifications in the training/testing phases**. \r\n",
        "\r\n",
        "Note: You cannot use other classification models (e.g., Convolutional Neural Networks-based image classifiers). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZKbtaXMWexH"
      },
      "source": [
        "# TO-DO: insert here the layout configuration that reached >0.85 of F1-score in\r\n",
        "# test set provided. Do not use models other than SVM, Log. Regression\r\n",
        "# and Gauss. Naive Bayes. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rCMM-FRkwYj"
      },
      "source": [
        "### **2.6 Custom dataset**\r\n",
        "\r\n",
        "Time to create and use your own dataset! Download images and divide them into train/test subfolders following the sample data provided. You can also use a subset of a pre-existing dataset. \r\n",
        "\r\n",
        "Requirements are: \r\n",
        "- 3+ classes\r\n",
        "- 70+ training images per class (.jpg files)\r\n",
        "- 10+ testing images per class (.jpg files)\r\n",
        "- Up to 50MB for the .zip file of your dataset  \r\n",
        "\r\n",
        "Calculate the three types of features (i.e., HOG, HHist and raw) and train the three types of models (SVM, Logistic Regression and Gaussian Naive Bayes) using the function you created. \r\n",
        "\r\n",
        "Evaluate your models on your custom dataset using the \"my_evaluation\" function and create a table such as the one in sub-section 2.4 (i.e., presenting the average precision, average recall and F1-score for each of the nine custom-trained models). \r\n",
        "\r\n",
        "Note: zip your dataset in a file called \"my_data.zip\" and add it to your submission package on BrightSpace so that we can reproduce your experimental setting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgkAABginnVx"
      },
      "source": [
        "# TO-DO: add the code for the reading, training of models and evaluation processes \r\n",
        "# using your custom dataset. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTNaGOx3beyg"
      },
      "source": [
        "**End of the assignment!**"
      ]
    }
  ]
}