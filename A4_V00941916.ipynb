{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A4_V00941916.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U0nrIg51CXFF"},"source":["![UVic logo](https://res-2.cloudinary.com/crunchbase-production/image/upload/c_lpad,h_256,w_256,f_auto,q_auto:eco/v1406151713/wptak6xuezyh36b1hbty.png)\n","\n","# **ECE 471/536 Spring 2021: Computer Vision**\n","## Assignment 4: Convolutional Neural Networks for Image Classification \n","### Due date: April 1st, 10:00 PM PST\n","\n","\n","> Student: Xiangyu Ren, V00941916\n","---\n","Abstract: *This assignment provides students with the opportunity to work with the various steps involved in the creation, training and evaluation of a convolutional neural network for image classification.*\n"," "]},{"cell_type":"markdown","metadata":{"id":"srHd9KhbFHcU"},"source":["## **1. Instructions:** follow the intructions provided on a sequential manner. \n","### 1.0 **Identification** \n","Please enter your name and V number on the text cell above.\n","\n","### 1.1 **Submission package**\n","Your final submission package must be submitted using the [BrightSpace](https://https://bright.uvic.ca/d2l/home)  platform. You will find this assignment's specific page under **Course Tools > Assignments**. Your submission package consists of a *.zip* file containing:\n","\n","1.   *.ipynb* file: your modified version of this Google Colab template. Place your complete assignment solution/information in this version. \n","\n","### 1.2 **Coding considerations**\n","* In previous years we asked students to complete assignments offline by installing either MATLAB or a Python environment in their computers. In order to standardize the submissions and guarantee that everyone has access to the same Python environment, all assignments are going to be described (by us) and completed (by you) using the same Google Colab template script.\n","* Google Colab offers a Python environment that can be accessed in your browser and executed using Google Cloud, so no local installation is necessary. It makes the setting-up process significantly easier! Please read [this quick tutorial](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb) notebook on Google Colab/Python.\n","\n","### 1.3 **Creating your Google Colab submission file**\n","\n","* Modify this template with your solutions to the assignment. You will find \"**TO-DO**\" indicators throughout the assingment highlighting portions of the code you are asked to complete, as well as their worth. \n","* Colab notebooks are divided into individual cells. You can execute the code inside of a given cell by pressing **CTRL+ENTER**, or that of all cells by pressing **CTRL+F9**. Variables must be \"executed\" in a cell before being used by subsequent ones (the same goes for libraries imported). Note that some cells of this assignment contain flags that must by changed (and executed) before you move forward.\n","* If you completed the whole assignment, make sure that simply pressing \"**CTRL+F9**\" executes all cells correctly. **This is going to be the first marking step we will execute when evaluating your submission**.  \n","\n","### 1.4 **Use of open source code**\n","\n","* The use of small segments of freely-available code is permitted. However, it is **extremely important** that you indicate in your in-code comments where these are used, as well as their sources. Failure to do so can be considered plagiarism, which is a serious offence. Learn more about detection mechanisms and consequences of plagiarism at UVic [here](https://www.uvic.ca/library/research/citation/plagiarism/). Note that the programming assignments are designed so that most of their content should be created by you.     \n","* A number of functions/algorithms are already implemented by libraries we will use (e.g., [OpenCV](https://opencv.org/), [scikit-learn](https://scikit-learn.org/stable/)), however you should not use them unless otherwise instructed to do so. Mannualy coding some of these function is an important part of the learning process. \n"]},{"cell_type":"code","metadata":{"id":"YQS1k9BUa_8Z","executionInfo":{"status":"ok","timestamp":1616624669914,"user_tz":420,"elapsed":592,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}}},"source":["READ_THE_INSTRUCTIONS_FLAG = True"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRhMh6tJjvcc"},"source":["# **2. Programming**"]},{"cell_type":"markdown","metadata":{"id":"0D4ecEgtjzOY"},"source":["### **2.1 Dataloading: getting the dataset ready** (worth up to 15 points)\n","\n","### *Preamble*\n","\n","In this assignment we are going to work with a real-world dataset called [MNIST](http://yann.lecun.com/exdb/mnist/). This popular dataset of handwritten digits contains 50,000 training samples and 10,000 testing samples. \n","\n","We are also going to work with the Python-based deep learning framework [Pytorch](https://pytorch.org/). Pytorch offers a number of useful functions for all stages of the design of a deep learning-based framework: dataset acquisition, dataloading, model implementation, backpropagation, pre-trained Convolutional Neural Networks, visualization, among others. You are encouraged to read Pytorch tutorials such as this and this before proceeding. \n","\n","Alongside with [TensorFlow](https://www.tensorflow.org/), Pytorch represents one of the most popular open source libraries used for machine learning/deep learning development and deployment.  \n","\n","### *Downloading and preparing the MNIST dataset*\n","\n","In this first segment of the assignment you are asked to download MNIST (using Pytorch functions) and create indices to divide it into training, validation and testing. These indices will drive the creation of dataloaders aiming to facilitate the access to batches of these three subsets (an important ability when working with larger datasets).  "]},{"cell_type":"code","metadata":{"id":"b_yOTMF-S_e-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616624672937,"user_tz":420,"elapsed":3601,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}},"outputId":"326f1add-e52a-4860-ebf8-b3065b2d3788"},"source":["import sys\n","import os\n","import torch # PyTorch \n","from torchvision import datasets  \n","import torchvision.transforms as transforms \n","from torch.utils.data.sampler import SubsetRandomSampler # Sampler \n","import numpy as np \n","\n","print('-'*40)\n","print ('Python version: {}'.format(sys.version))\n","\n","if not READ_THE_INSTRUCTIONS_FLAG:\n","  raise Exception('Please go back and read the instructions.')\n","else:\n","  print('\\nThank you for reading the instructions.')\n","print('-'*40)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["----------------------------------------\n","Python version: 3.7.10 (default, Feb 20 2021, 21:17:23) \n","[GCC 7.5.0]\n","\n","Thank you for reading the instructions.\n","----------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m-C7cbxjx0MK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616624677849,"user_tz":420,"elapsed":8507,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}},"outputId":"64eeac9f-68df-442d-8c91-eb6087e006ed"},"source":["# TO-DO: use torchvision's \"datasets\" to download the MNIST dataset. You are asked to \n","# download two sets: \"mnist_train\" and \"mnist_test\". \n","# tip: the variable \"train\" in the MNIST-downloading function determines the segment of the \n","# dataset you are trying to download (i.e., train or test)\n","# Note: we are going to read the data as Tensors (similar to numpy arrays, but they are \n","# more adequate to handle using either the CPU or GPU—as we know, GPUS are the \n","# preferred processing unit when training neural networks). Therefore, when reading the dataset, \n","# set the parameter \"transform\" such that the data is read as a Tensor. \n","\n","root = './data'\n","use_cuda = torch.cuda.is_available()\n","\n","if not os.path.exists(root):\n","  os.mkdir(root)\n","trans = transforms.Compose(transforms.ToTensor())\n","# mnist_train = datasets.MNIST(root=root,train=True, download=trans, transform=None) # TO-DO\n","# mnist_test = datasets.MNIST(root=root,train=False, download=trans, transform=None) # TO-DO\n","\n","# Note: you will try to download the MNIST dataset from Yann Lecun's webiste. If it is unavailable, you can use this \n","# alternative source: \n","!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n","!tar -zxvf MNIST.tar.gz \n","PATH = './MNIST/processed/'\n","mnist_train = torch.load(PATH+'training.pt')\n","mnist_test = torch.load(PATH+'test.pt')\n","# A sucessful download of MNIST should look like: https://raw.githubusercontent.com/tunai/storage/master/images/teaching/ece%20473-536/A4/downloading_the_dataset.jpg \n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2021-03-24 22:24:32--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n","Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n","Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n","--2021-03-24 22:24:33--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n","Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/x-gzip]\n","Saving to: ‘MNIST.tar.gz’\n","\n","MNIST.tar.gz            [          <=>       ]  33.20M  15.2MB/s    in 2.2s    \n","\n","2021-03-24 22:24:35 (15.2 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n","\n","MNIST/\n","MNIST/raw/\n","MNIST/raw/train-labels-idx1-ubyte\n","MNIST/raw/t10k-labels-idx1-ubyte.gz\n","MNIST/raw/t10k-labels-idx1-ubyte\n","MNIST/raw/t10k-images-idx3-ubyte.gz\n","MNIST/raw/train-images-idx3-ubyte\n","MNIST/raw/train-labels-idx1-ubyte.gz\n","MNIST/raw/t10k-images-idx3-ubyte\n","MNIST/raw/train-images-idx3-ubyte.gz\n","MNIST/processed/\n","MNIST/processed/training.pt\n","MNIST/processed/test.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QkgJb71gaB74","executionInfo":{"status":"ok","timestamp":1616624677850,"user_tz":420,"elapsed":8504,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}}},"source":["PATH = './MNIST/processed/'\n","mnist_train = torch.load(PATH+'training.pt')\n","mnist_test = torch.load(PATH+'test.pt')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WiW8wENFarR1","executionInfo":{"status":"ok","timestamp":1616624678016,"user_tz":420,"elapsed":8665,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}},"outputId":"735a5da5-3e84-4e4a-9aab-bce3e38a1d7e"},"source":["mnist_test[0].shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10000, 28, 28])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"kf0smZdM1G7J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616624678017,"user_tz":420,"elapsed":8660,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}},"outputId":"d61624c3-f894-45ef-a8b0-0159b79aa1e4"},"source":["# In this cell you will partition the train set into \"train\" and \"validation\". \n","# We already have a training and testing set. As discussed in class, a good practice is \n","# to also create a validation subset (to be used during training to avoid overfitting). \n","import random\n","import math\n","\n","idx = list(range(mnist_train[0].shape[0]))  #TO-DO: create a list of indices with lenght equal to the number of training samples\n","# e.g., for 5 training samples, the idx list would be \"01234\"\n","\n","# TO-DO: shuffle the elements of this indices list\n","random.seed(6)\n","random.shuffle(idx)\n","print(\"First 20 shuffled indices: {}\".format(idx[0:19]))\n","# Note: make sure that the indices printed are shuffled. \n","\n","# Divide the \"idx\" list of shuffled indices into 70% for \"training_idx\" and \n","# 30% for \"validation_idx\"\n","# e.g., idx = 0 72 12 48 39 85 120 3 1 98\n","# training_idx would be the first 70% indices: 0 72 12 48 39 85 120  \n","# validation_idx would be the remaning 30% indices: 3 1 98 \n","data_split = math.floor(0.7* len(idx))\n","\n","\n","train_idx = idx[0:data_split] #TO-DO\n","validation_idx = idx[data_split:] #TO-DO"],"execution_count":6,"outputs":[{"output_type":"stream","text":["First 20 shuffled indices: [31906, 19640, 6348, 15654, 11507, 57019, 14199, 37367, 42484, 9891, 13998, 34543, 28905, 20794, 56344, 41692, 37977, 58813, 5885]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AWfQbeRd5JcV","colab":{"base_uri":"https://localhost:8080/","height":382},"executionInfo":{"status":"error","timestamp":1616626846852,"user_tz":420,"elapsed":420,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}},"outputId":"e589fb4c-098e-4983-dfd1-30d1bf1eaaf2"},"source":["# Now we are going to use dataloaders to get the data ready to be used with Pytorch models. You will use Pytorch's  \n","# \"SubsetRandomSampler\" module, as well as its Dataloader functions. \n","from torch.utils.data import DataLoader\n","\n","# TO-DO: define \"samplers\". These are lists specifying the shuffled indices we are going to use when reading the \n","# previously downloaded MNIST dataset (you created these shuffled lists in the last cell). \n","\n","train_smp = SubsetRandomSampler(train_idx) #TO-DO: use Pytorch's \"SubsetRandomSampler\" to create a sampler based on your \"train_idx\" list\n","validation_smp = SubsetRandomSampler(validation_idx) #TO-DO: use Pytorch's \"SubsetRandomSampler\" to create a sampler based on your \"validation_idx\" list\n","\n","# TO-DO: use Pytorch's \"Dataloader\" function to create loaders (i.e., structures that help handling the data even in \n","# arbitrarily large datasets). Note that the division in your training set is going to be given by the samplers that \n","# you just created.\n","# note: set the input parameter \"num_workers\" to zero (this can be used to have multiple subprecesses involved in the \n","# dataloading) \n","\n","bsize = 16 # number of images to be considered at a time (i.e., \"minibatch\" from class 14)\n","\n","train_loader = DataLoader(mnist_train, batch_size=bsize, sampler=train_smp, ) #TO-DO (create a loader using PyTorch's DataLoader function, mnist_train and your training_smp)\n","validation_loader = DataLoader(mnist_train, batch_size=bsize, sampler=validation_smp) #TO-DO (create a loader using PyTorch's DataLoader function, mnist_train and your validation_smp)\n","test_loader = DataLoader(mnist_test, batch_size=bsize) #TO-DO (create a loader using PyTorch's DataLoader function and mnist_test)\n","\n","# Debugging the dataloading process. \n","# In order to make sure that the dataloading process was correctly completed, we will read and display\n","# a number of datapoints from the training set. \n","\n","# Dataloaders make the datareading process extremely easy:\n","  \n","datal = [] #TO-DO: grab an entire dataloader of training images and their respective labels\n","\n","for inputs, targets in train_loader:\n","    print(inputs)\n","    print(targets)\n","\n","\n","imgs, labels = datal.next()  #TO-DO: from the dataloader, grab a single minibatch of images and their labels  \n","\n","# TO-DO: use matplotlib to display a minibatch of images and their respective labels.\n","# Your plot should look like this: https://raw.githubusercontent.com/tunai/storage/master/images/teaching/ece%20473-536/A4/sample_training_minibatch.jpg  \n","# tip 1: you might need to change the format of the images before displaying them with \n","# matplotlib\n","# tip 2: when reading individual images from the dataloader, they are considered to be \n","# (N,H,W), where N represents the number of samples (1), thus your dimensions are 1x28x28 (for\n","# mnist samples). You have to deal with this extra dimension before plotting a sample.\n","plt.figure()\n","bsize = 1\n","for i in range(bsize):\n","  plt.subplot(bsize,i)\n","  fig = reshape(DataLoader[i],[imgs[i].shape[1],imgs[i].shape[2]])\n","  plt.imshow(fig)"],"execution_count":13,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-a494a1cc2960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdatal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#TO-DO: grab an entire dataloader of training images and their respective labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"]}]},{"cell_type":"markdown","metadata":{"id":"OKHhpN0oC1Gj"},"source":["### **2.2 Convolutional Neural Network creation** (worth up to 25 points)\n","\n","We are going to create a small Convolutional Neural Network (CNN) to perform an  image classification task on the handwritten digits dataset. This network is specified by defining its architecture and forward passes (the backward pass, where backpropagation happens, is automatically implemented by torch.autograd). \n","\n","In this segment you will define a custom CNN [(architecture)](https://raw.githubusercontent.com/tunai/storage/master/images/teaching/ece%20473-536/A4/model.jpg) and perform an initial analysis on its ability to receive a batch of inputs and create corresponding (untrained) predictions.     \n","\n"]},{"cell_type":"code","metadata":{"id":"TVAXi82aG5xc","executionInfo":{"status":"aborted","timestamp":1616624678404,"user_tz":420,"elapsed":9038,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","#TO-DO: time to build your own CNN! The template of the \"my_network\" function will walk you through its creation process. \n","# the architecture that you are trying to reproduce is the following: https://raw.githubusercontent.com/tunai/storage/master/images/teaching/ece%20473-536/A4/model.jpg  \n","\n","class my_network(nn.Module):\n","\n","    def __init__(self): # this function will detail the architecture of your CNN\n","        super(my_network, self).__init__()\n","\n","        # Define the convolution and fully connected (FC) layers (note: pooling and \n","        # activation functions are going to be defined only in the forward pass—next function)  \n","        \n","        # First, two convolution layers: conv1 and conv2\n","        # conv1 takes the input (1x28x28 tensors), uses 3x3 conv. kernels and outputs \n","        # 6 channels. \n","        # conv 2 takes the 6 channels from the previous layer and uses 3x3 conv kernel \n","        # to output 12 channels\n","        # tip: use nn.Conv2d\n","\n","        # TO-DO: define the convolutional layers of the network (e.g., self.conv1)\n","        self.conv1 = nn.Conv2d(1,6,3,1)\n","        self.conv2 = nn.Conv2d(6,12,3,1)\n","\n","        # Second, define the fully connected layers at the end of the network (i.e., \n","        # the neural network at the end of the CNN).\n","        # Your neural network will have four FC layers. \n","        \n","        # fc1: The input of the first FC layer is going to be given by the dimensions\n","        # of the input image after going throught two convolution operations (as well as the \n","        # max poolings) times the number of channels coming from the conv2 layer. As discussed in class, you\n","        # can use the following formula to calculate the dimensions of an input after a CONVOLUTION takes \n","        # place (you will also have to consider the max pooling—see the \"forward\" function below): \n","\n","        # ([(W - K + 2P)/S] + 1)\n","        # W = Input size\n","        # K = Filter size\n","        # S = Stride \n","        # P = Padding \n","\n","        # Set the OUTPUT of fc1 to 140 dimensions\n","        # fc2: output = 80 dimensions\n","        # fc3: output = 40 dimensions\n","        # fc4: output = 10 dimensions \n","\n","        # tip: use nn.Linear\n","\n","        # TO-DO: define the fully connected layers of the network (e.g., self.fc1)\n","        self.fc1 = nn.Linear(,140)\n","        self.fc2 = nn.Linear(140,80)\n","        self.fc3 = nn.Linear(80,40)\n","\n","\n","\n","       \n","    def forward(self, data):\n","\n","        # The forward pass will specify the path that a tensor does from \n","        # input to output. We will use the layers we already defined and \n","        # connect them with pooling and activation layers\n","\n","        # First, define BLOCK 1, which receives the input, passess it through \n","        # the conv1 layer, applies a ReLU in the result, and then a max pooling layer\n","        # in the result of the ReLU. In other words, input -> conv1 -> ReLU -> max pool\n","        # Note: use a (2,2) kernel for the max pooling with stride=2\n","        # tips: F.max_pool2d, F.relu\n","\n","        # Second, use the output of this first block in a similar BLOCK 2 of:\n","        # conv2 -> ReLU -> max pool (again, (2,2) windows on max pool and stride=2)\n","\n","        # TO-DO:\n","        # define BLOCK 1\n","        # use the output of BLOCK 1 to calculate the output of BLOCK 2  \n","\n","        # Take a moment to manually determine what are the dimensions of the data at this point (i.e., CxhxW). \n","        # These dimensions are important for the calculation of the input size of fc1 (see above).\n","        # Tip: when using max pool with a (2,2) kernel and stride=2 on a square images with an \n","        # odd number of pixels (e.g., 15x15), consider the integer in the output dimensions. \n","        # e.g., 31 x 31 -> max pool (2,2) stride 2 -> 15 x 15 \n","\n","        # TO-DO: flatten the data so that it can be used in the first FC layer.\n","        # flattened = #TO-DO\n","\n","        # In this solution you can use the \"num_flat_features\" helper function provided to calculate the number of \n","        # features from a single sample, which helps the flattening process.\n","        # mannualy, the calculations would be: \n","        # input: 28 x 28\n","        # output of block 1: 13 x 13\n","        # output of block 2: 5 x 5 \n","        # number of elements: 5 x 5 x 12 = 300\n","\n","        # TO-DO: use the flattened data as the input of the first fc1, and apply a ReLU \n","        # after it. \n","        # Do the same process for fc2 and fc3 (i.e., apply ReLU to the output).\n","        # You do not need to apply ReLU to the output of fc4 (these are the predictions \n","        # of the system—they might take negative values)\n","\n","        return out # return the output of the forward pass\n","\n","# model = TO-DO # create o model using the architecture you just defined\n","\n","print(model)\n","\n","# Tip: you can use the summary function (!pip install torchsummary; from torchsummary import summary)\n","# to help you make sure that your model's architecture is correct.\n","# If this function runs correctly, your model is ready to receive a batch input \n","# https://github.com/sksq96/pytorch-summary \n","# Reference output: https://raw.githubusercontent.com/tunai/storage/master/images/teaching/ece%20473-536/A4/summary_fnc.jpg\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dNCYUvysdWIY","executionInfo":{"status":"aborted","timestamp":1616624678406,"user_tz":420,"elapsed":9037,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}}},"source":["# Initial testing of the network\n","\n","# TO-DO: use your function to make predictions in one minibatch of the training set. Present the \n","# predictions of the model (since the model was not yet trained, they will be meaningless numbers)\n","# imgs = TO-DO: grab the images of a batch\n","# labels = TO-DO: grab the labels of this same batch\n","# TO-DO: print the shape of the minibatch\n","# note: you can use a minibatch from train, validation or test at this stage\n","\n","# pred = TO-DO: perform a set of predictions using your model and print them  \n","# Note: Pytorch expects minibatches as inputs (i.e., nSamples x nChannels x Height x Width).\n","# If you working with a single image, you have to add one dimension to simulate a minibatch. \n","# You can use input.unsqueeze(0) to accomplish that.  \n","\n","# TO-DO: create a function that receives raw values (i.e., predictions of your model)\n","# and turns them into probabilities. \n","# Recall from lecture 13 that the Softmax function is able to do that. \n","\n","def my_softmax(logits):\n","  ## TO-DO implement softmax\n","  return probs # return normalized probabilities\n","\n","# TO-DO: use your my_softmax function to print the probabilities associated with the \n","# predictions of your model for the sample minibatch that you calculated earlier in this cell."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8eK3jIW0-DXU"},"source":["### **2.3 Calculating the losses mannualy and automatically** (worth up to 10 points)\n","\n","As discussed during Lectures 13 and 14, loss functions are able to calculate how close to a given ground truth value a set of predictions are. The losses calculated serve as the basis for backpropagation methods, thus their importance. \n","\n","In this segment you are asked to create a function to manually calculate the cross-entropy loss of a set of predictions and compare it with that calculated by Pytorch. You will also calculate the SVM Multiclass loss for illustration purposes. \n"]},{"cell_type":"code","metadata":{"id":"rJc3sZogwfPy","executionInfo":{"status":"aborted","timestamp":1616624678407,"user_tz":420,"elapsed":9035,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}}},"source":["from math import isclose\n","\n","# the \"nn\" module of Pytorch implementes a number of losses for classification and regression problems. We are going to \n","# explore the use of both Multiclass SVM and Cross-entropy loss. \n","\n","probs = TO-DO # a \"probabilities\" version of your predictions. Refer to the last cell \n","# for the implementation of \"my_softmax\"\n","\n","# TO-DO: implement the cross_entropy loss mannualy using the \"my_cross_entropy\" function. \n","# Recall that the loss calculated by this function will be the mean of the cross-entropy loss\n","# considering all the elements of a minibatch.\n","\n","def my_cross_entropy(probs, labels):\n","  \n","  # This function receives probabilites and labels, and returns a single scalar \n","  # representing the mean cross-entropy loss. \n","\n","  return loss\n","\n","# TO-DO: use your \"my_cross_entropy\" function to calculate the cross-entropy loss\n","# of pred (which you calculated earlier in this cell) based on the correct labels.\n","# Print both the results of your \"my_cross_entropy\" loss function and the \n","# cross-entropy loss value \n","# calculated using Pytorch's \"nn.CrossEntropyLoss\"\n","\n","manual_ce_loss = #TO-DO # use your function\n","\n","# reference CE loss: now use Pytorch's function to calculate the cross-entropy loss\n","reference_ce_loss = #TO-DO: calculate CE loss using Pytorch's function\n","\n","# Assertion to make sure that your manual CE loss yelds similar values to the automatic one. \n","assert (isclose(reference_ce_loss,manual_ce_loss,abs_tol=1e-2)), 'The manual and automatically calculated CE losses are too different.'\n","\n","# TO-DO: Calculate and print the Multiclass SVM loss for this same set of predictions.  \n","# Tip: Pytorch's \"MultiMarginLoss\"\n","multiclass_svm_loss = #TO-DO: calculate multiclass svm loss using Pytorch's function\n","\n","print('Manual cross-entropy loss: {}\\nReference cross-entropy loss: {}\\nMulticlass SVM Loss:{}'.format(manual_ce_loss,reference_ce_loss,multiclass_svm_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-5wYRUeE-Ld3"},"source":["### **2.4 Training your CNN** (worth up to 40 points + 5 extra points)\n","\n","Togheter with the creation of a network, the training routine represents the most important aspect of a DL-based image classification framework. \n","\n","In this segment, you will implement a training loop that predicts scores from batches of samples from the training set, and based on the losses (and gradients) calculated, updates the values of the network's parameters. \n"]},{"cell_type":"code","metadata":{"id":"gHvLVbNgFcV_","executionInfo":{"status":"aborted","timestamp":1616624678409,"user_tz":420,"elapsed":9036,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}}},"source":["# Before starting the training, observe in this illustrative cell the process used\n","# to log the statistics of your training using TensorBoard in Google Colab.\n","# This is an extremely useful way to make sure that your training process is \n","# progressing correctly (i.e., not overfitting or stuck in the optimization process)\n","\n","from torch.utils.tensorboard import SummaryWriter\n","%load_ext tensorboard\n","import os\n","import shutil\n","\n","logs_base_dir = \"runs\" # directory where the logs are going to be saved\n","if os.path.exists('runs/'):\n","  shutil.rmtree('runs/')\n","os.makedirs(logs_base_dir, exist_ok=True)\n","\n","logger = SummaryWriter() # this is the object that is going to log your statistics.\n","# it can be used to log images, models, as well as simple scalars (our case)\n","\n","# Consider the simple example below. We want to save and present:\n","# 1 - a title for the graph \n","# 2 - the value of the scalar at index t\n","# 3 - index t\n","for i in range(1, 10): # add 20 sample scalars\n","    logger.add_scalar(\"Powers of two\", 2**i, i) # adds the title, value and index to the logger\n","\n","logger.close() # closes the logger to get it ready to present the values\n","\n","%tensorboard --logdir {logs_base_dir}\n","# note: clear the logs in your \"run\" folder before starting to avoid clutter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ut62MPO9K6W","executionInfo":{"status":"aborted","timestamp":1616624678409,"user_tz":420,"elapsed":9033,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}}},"source":["# In this cell you are going to code the training routine of your model\n","\n","import torch.optim as optim\n","\n","# Before starting the training process, you need to define the optimizer and loss function\n","# to be used. We are going to use ADAM as an optimizer, and cross-entropy loss as a loss function. \n","\n","# TO-DO: define the loss function to be used (\"criterion\") as well as the otimizing strategy (\"optimizer\").\n","# tip: use Pytorch's \"nn\" and \"optim\" modules. \n","\n","criterion = # TO-DO define the loss function (your choice! use one of the options discussed in class)\n","optimizer = # TO-DO define the opmitmizer (your choice! use one of the options discussed in class)\n","\n","learning_rate = # TO-DO: determines the step size for the optimization algorithm (tip: start with 0.001)\n","n_epochs = # TO-DO: number of times the whole (shuffled) dataset is going to be visited (tip: start with a small number—this is \n","# a simple dataset that should not take more than a couple of epochs to train efficiently)\n","\n","# TO-DO: create the \"performance_on_validation\" function to calculate the performance of your model\n","# on the validation set during the training routine. \n","\n","def performance_on_validation(validation_loader): # receives the validation set data loader as input \n","\n","    nValSamples = TO-DO: calculates the number of validation samples from the dataloader\n","\n","    \n","    # create a validation accuracy variable\n","    val_acc = 0.0\n","\n","    # Create a for loop that visits all minibatches of the validation set. \n","    # for .. TO-DO:\n","        outputs = TO-DO : predicts the scores of a minibatch (i.e., uses your model)\n","        prediction = TO-DO: based on the scores, predicts a class for each of the samples in the minibatch \n","        val_acc += TO-DO: sum to the val_acc variable the accuracy calculated in for this minibatch\n","        \n","        # Tip: do not worry about FP, TP, etc. in this context. This is simply a calculation of the number of \n","        # correct predictions by the number of predictions. Example: \n","        # Ground truth (labels): 03534\n","        # Predictions: 03134\n","        # Correct predictions: 4\n","        # Total predictions: 5\n","        # Acc for this minibatch: 4/5 \n","\n","    final_acc = #TO-DO: calculate the final accuracy (i.e., considering the accuracy of all minibatches)\n","\n","    return final_acc\n","\n","# TRAINING LOOP:\n","# Loop through different minibatches of your data. For each minibatch, calculate the \n","# model's predictions, loss, and update the model parameters\n","\n","best_val_acc = 0\n","overall_batch = 0\n","val_interval = 500 # the number of minibatches before you do a round of performance \n","# evaluation on your validation set\n","\n","# Create a loop that repeats \"n_epochs\" number of times\n","\n","# for... TO-DO (loop epochs)\n","\n","    current_loss = 0.0\n","    current_acc = 0.0\n","    \n","    # for ... TO-DO # this loop visits all minibatches of your dataset\n","        \n","        inputs = TO-DO: grab images of a minibatch\n","        labels = TO-DO: grab labels of a minibatch\n","\n","        # TO-DO: set all the current gradients for the parameters of your model to zero (so that a new backpropagation can be executed)\n","        # tip: the \"optmizer\" variable object has a function that does that\n","\n","        outputs = # TO-DO: use your model to get the predictions for the current minibatch \n","        loss = # TO-DO: calculate the loss for the predictions \n","        \n","        # TO-DO: perform backprop on the loss (i.e., gradient of the loss w.r.t. the parameters)\n","        # tip: you DO NOT NEED to implement backprop. Your loss variable will have a function that does that. \n","\n","        # TO-DO: update the parameters of your model using the optimization algorithm. \n","        # tip: you DO NOT NEED to implement the optimization. Your optimizer variable will have a function that does that. \n","\n","        current_loss += # TO-DO: this variable represents the running loss for a minibatch. \n","        prediction = # TO-DO: calculate the prediction based on the scores \n","        current_acc += # TO-DO: this variable represents the runnig accuracy for a minibatch (see the \n","        # \"performance_on_validation\" function)\n","\n","        # At each \"val_interval\" number of mini-batches, calculate the performance of your model on the validation set\n","        # if ... TO-DO\n","          # TO-DO: set your model into \"eval\" mode. That will prevent any parameters from being changed. This is the \n","          # desired behaviour when working with the validation set (remember, the validation set is not used to update the \n","          # model parameters, but rather to evaluate its performance in \"real-world\" data during training)       \n","\n","          val_acc = TO-DO: use your \"performance_on_validation\" function to calculate the validation accuracy at this training \n","          # iteration \n","\n","          # TO-DO: set your model back to \"train\" mode\n","          \n","          # If the validation on the val set is the highest seem so far, update the best model's parameters\n","          # if ... TO-DO\n","            print('Updated the best model!')\n","            best_val_acc=val_acc\n","            best_model = TO-DO: save the model's state dictionary (i.e., parameters). Call it 'best_model.pth'\n","            # tip: torch.save\n","          \n","          # Some prints to help you debbug your training: \n","          #print('Epoch {}, Iteration {} statistics:'.format(epoch_number,i))\n","          #print('Train acc: {}\\nTrain loss:{}\\nVal acc:{}\\n'.format((current_acc/i)*100,current_loss/i,val_acc*100))\n","\n","\n","# (EXTRA TO-DO: 5 Points): Use tensorboard to show the training accuracy, training loss and validation accuracy every \n","#  \"val_interval\" number of mini-batches. \n","# Tip: refer to the last cell for a reference usage of tensorboard. \n","# Note: this is not mandatory—these 5 points can only help you achieve 100% in this assignment.\n","\n","print('End of the training loop!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1C2gEdtBIFS"},"source":["### **2.5 Testing and displaying the detection results** (worth up to 10 points)\n","\n","The last segment of this assignment involves the calculation of the performance of your trained model. In order to qualitatively evaluate your model, you will also create a function that receives a batch of samples, predicts their classes and displays the images, labels and predictions. "]},{"cell_type":"code","metadata":{"id":"IsPXI5SWH2Rv","executionInfo":{"status":"aborted","timestamp":1616624678411,"user_tz":420,"elapsed":9033,"user":{"displayName":"xiangyu ren","photoUrl":"","userId":"08366319851288904396"}}},"source":["# TO-DO: Run your trained model on the whole test set of MNIST and print the results \n","\n","test_performance = #TO-DO # Calculate accuracy of the trained model on the test set (all minibatches considered!)\n","# Your test accuracy has to be higher than 0.96. \n","assert(test_performance>0.96), 'Your test accuracy is too low!'\n","\n","# TO-DO: Create a function that receives n batches of images, performs and displays the \n","# predictions of the trained model and the labels. \n","# Sample output (for a single epoch of training and number of batches = 5): \n","# https://raw.githubusercontent.com/tunai/storage/master/images/teaching/ece%20473-536/A4/5_batches_pred_wrong_highlighted.jpg\n","# Your output should look like the sample output. Note: your predictions/samples are going to be different! Only the overall \n","# displaying template should follow the sample output. \n","\n","def predict_and_display(model,loader,n_batches=1): \n","  # inputs:\n","  # model = your trained model\n","  # loader = test set dataloader \n","  # n_batches = number of batches you are going to do prediction on\n","  \n","  batches = #TO-DO: get all the batches from the test dataloader\n","  \n","  # Loop for \"n_batches\" iterations\n","  # for ... TO-DO:\n","    imgs = #TO-DO: grab images of a minibatch\n","    labels = #TO-DO: grab labels of a minibatch\n","    scores = #TO-DO: calculate the scores of a minibatch\n","    pred = #calculate the preds of a minibatch based on the scores    \n","    \n","    # TO-DO: use matplotlib to show all images from a minibatch in a single row (see reference above).\n","    # on top of each image you should display the label and prediction for that sample: \"L:#/P:#\" (see reference above)   \n","\n","\n","# use your \"predict_and_display\" function to display the predictions, images and labels \n","# of all samples from 5 minibatches at the same time (see reference above). "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTNaGOx3beyg"},"source":["**End of the assignment!**"]}]}